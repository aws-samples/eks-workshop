[
{
	"uri": "/",
	"title": "Amazon EKS Workshop",
	"tags": [],
	"description": "",
	"content": "Amazon EKS Workshop In this workshop, we will explore multiple ways to configure VPC, ALB, and EC2 Kubernetes workers, and Amazon Elastic Container Service for Kubernetes.\n"
},
{
	"uri": "/prerequisites/account/",
	"title": "Create an AWS account",
	"tags": [],
	"description": "",
	"content": "Your account must have the ability to create new IAM roles and scope other IAM permissions.\n  If you don\u0026rsquo;t already have an AWS account with Administrator access: create one now by clicking here\n Once you have an AWS account, ensure you are following the remaining workshop steps as an IAM user with administrator access to the AWS account: Create a new IAM user to use for the workshop\n Enter the user details:  Attach the AdministratorAccess IAM Policy:  Click to create the new user:  Take note of the login URL and save:   "
},
{
	"uri": "/conclusion/conclusion/",
	"title": "What Have We Accomplished",
	"tags": [],
	"description": "",
	"content": "We have:\n Deployed an application consisting of microservices Deployed the Kubernetes Dashboard Deployed packages using Helm Deployed a centralized logging infrastructure Configured Automatic scaling of our pods and worker nodes  "
},
{
	"uri": "/monitoring/prereqs/",
	"title": "Prereqs",
	"tags": [],
	"description": "",
	"content": " Is helm installed? We will use helm to install Prometheus \u0026amp; Grafana monitoring tools for this chapter. Please review installing helm chapter for instructions if you don\u0026rsquo;t have it installed.\nhelm ls  Configure Storage Class We will use gp2 EBS volumes for simplicity and demonstration purpose. While deploying in Production, you would use io1 volumes with desired IOPS and increase the default storage size in the manifests to get better performance.\nkubectl create -f - \u0026lt;\u0026lt;EOF { \u0026quot;kind\u0026quot;: \u0026quot;StorageClass\u0026quot;, \u0026quot;apiVersion\u0026quot;: \u0026quot;storage.k8s.io/v1\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;prometheus\u0026quot;, \u0026quot;namespace\u0026quot;: \u0026quot;prometheus\u0026quot; }, \u0026quot;provisioner\u0026quot;: \u0026quot;kubernetes.io/aws-ebs\u0026quot;, \u0026quot;parameters\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;gp2\u0026quot; }, \u0026quot;reclaimPolicy\u0026quot;: \u0026quot;Retain\u0026quot;, \u0026quot;mountOptions\u0026quot;: [ \u0026quot;debug\u0026quot; ] } EOF  "
},
{
	"uri": "/deploy/applications/",
	"title": "Deploy our Sample Applications",
	"tags": [],
	"description": "",
	"content": "apiVersion: apps/v1 kind: Deployment metadata: name: ecsdemo-nodejs labels: app: ecsdemo-nodejs namespace: default spec: replicas: 1 selector: matchLabels: app: ecsdemo-nodejs strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app: ecsdemo-nodejs spec: containers: - image: brentley/ecsdemo-nodejs:latest imagePullPolicy: Always name: ecsdemo-nodejs ports: - containerPort: 3000 protocol: TCP  In the sample file above, we describe the service and how it should be deployed. We will write this description to the kubernetes api using kubectl, and kubernetes will ensure our preferences are met as the application is deployed.\nThe containers listen on port 3000, and native service discovery will be used to locate the running containers and communicate with them.\n"
},
{
	"uri": "/helm/install/",
	"title": "Install Helm CLI",
	"tags": [],
	"description": "",
	"content": "Before we can get started configuring helm we\u0026rsquo;ll need to first install the command line tools that you will interact with. To do this run the following.\ncd ~/environment curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get \u0026gt; get_helm.sh chmod +x get_helm.sh ./get_helm.sh  "
},
{
	"uri": "/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Introduction to Kubernetes A walkthrough of basic Kubernetes concepts.\nWelcome to the Amazon EKS Workshop!\nThe intent of this workshop is to educate users about the features of Amazon EKS.\nBackground in EKS, Kubernetes, Docker, and container workflows are not required, but they are recommended.\nThis chapter will introduce you to the basic workings of Kubernetes, laying the foundation for the hands-on portion of the workshop.\nSpecifically, we will walk you through the following topics:\n Kubernetes (k8s) Basics   Kubernetes Architecture   Amazon EKS   "
},
{
	"uri": "/servicemesh/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Istio  Let\u0026rsquo;s review in detail what each of these components are, It is a completely open source service mesh that layers transparently onto existing distributed applications. It is also a platform, including APIs that let it integrate into any logging platform, or telemetry or policy system.\n  Envoy\n Process the inbound/outbound traffic from inter-service and service-to-external-service transparently.  Pilot\n Pilot provides service discovery for the Envoy sidecars, traffic management capabilities for intelligent routing (e.g., A/B tests, canary deployments, etc.), and resiliency (timeouts, retries, circuit breakers, etc.)  Mixer\n Mixer enforces access control and usage policies across the service mesh, and collects telemetry data from the Envoy proxy and other services.  Citadel\n Citadel provides strong service-to-service and end-user authentication with built-in identity and credential management.   "
},
{
	"uri": "/monitoring/deploy-prometheus/",
	"title": "Deploy Prometheus",
	"tags": [],
	"description": "",
	"content": " Download Prometheus curl -o prometheus-values.yaml https://raw.githubusercontent.com/helm/charts/master/stable/prometheus/values.yaml  Open the prometheus-values.yaml you downloaded by double clicking on the file name on the left panel.\nSearch for # storageClass: \u0026ldquo;-\u0026rdquo; in the prometheus-values.yaml, uncomment and change the value to \u0026ldquo;prometheus\u0026rdquo;. You will do this twice, under both server \u0026amp; alertmanager manifests\nThe manifests will look like below\n## Prometheus server data Persistent Volume Storage Class ## If defined, storageClassName: \u0026lt;storageClass\u0026gt; ## If set to \u0026quot;-\u0026quot;, storageClassName: \u0026quot;\u0026quot;, which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS \u0026amp; OpenStack) ## storageClass: \u0026quot;prometheus\u0026quot;  ## alertmanager data Persistent Volume Storage Class ## If defined, storageClassName: \u0026lt;storageClass\u0026gt; ## If set to \u0026quot;-\u0026quot;, storageClassName: \u0026quot;\u0026quot;, which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS \u0026amp; OpenStack) ## storageClass: \u0026quot;prometheus\u0026quot;  Search for ## List of IP addresses at which the Prometheus server service is available, add nodePort: 30900 and change the type to NodePort as indicated below. Because Prometheus is exposed as ClusterIP by default, the web UI cannot be reached outside of Kubernetes. The reason we are adding NodePort here is for viewing the web UI from worker node IP address. This configuration is not recommended in Production and there are better ways to secure it. You can read more about exposing Prometheus web UI in this link\n## List of IP addresses at which the Prometheus server service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 80 nodePort: 30900 type: NodePort  Deploy Prometheus helm install -f prometheus-values.yaml stable/prometheus --name prometheus --namespace prometheus  Make a note of prometheus endpoint in helm response (you will need this later). It should look similar to below\nThe Prometheus server can be accessed via port 80 on the following DNS name from within your cluster: prometheus-server.prometheus.svc.cluster.local  Check if Prometheus components deployed as expected\nkubectl get all -n prometheus  You should see response similar to below. They should all be Ready and Available\nNAME READY STATUS RESTARTS AGE pod/prometheus-alertmanager-77cfdf85db-s9p48 2/2 Running 0 1m pod/prometheus-kube-state-metrics-74d5c694c7-vqtjd 1/1 Running 0 1m pod/prometheus-node-exporter-6dhpw 1/1 Running 0 1m pod/prometheus-node-exporter-nrfkn 1/1 Running 0 1m pod/prometheus-node-exporter-rtrm8 1/1 Running 0 1m pod/prometheus-pushgateway-d5fdc4f5b-dbmrg 1/1 Running 0 1m pod/prometheus-server-6d665b876-dsmh9 2/2 Running 0 1m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/prometheus-alertmanager ClusterIP 10.100.89.154 \u0026lt;none\u0026gt; 80/TCP 1m service/prometheus-kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 80/TCP 1m service/prometheus-node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 1m service/prometheus-pushgateway ClusterIP 10.100.136.143 \u0026lt;none\u0026gt; 9091/TCP 1m service/prometheus-server NodePort 10.100.151.245 \u0026lt;none\u0026gt; 80/30900 1m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/prometheus-node-exporter 3 3 3 3 3 \u0026lt;none\u0026gt; 1m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/prometheus-alertmanager 1 1 1 1 1m deployment.apps/prometheus-kube-state-metrics 1 1 1 1 1m deployment.apps/prometheus-pushgateway 1 1 1 1 1m deployment.apps/prometheus-server 1 1 1 1 1m NAME DESIRED CURRENT READY AGE replicaset.apps/prometheus-alertmanager-77cfdf85db 1 1 1 1m replicaset.apps/prometheus-kube-state-metrics-74d5c694c7 1 1 1 1m replicaset.apps/prometheus-pushgateway-d5fdc4f5b 1 1 1 1m replicaset.apps/prometheus-server-6d665b876 1 1 1 1m  You can access Prometheus server URL by going to any one of your Worker node IP address and specify port :30900/targets (for ex, 52.12.161.128:30900/targets. Remember to open port 30900 in your Worker nodes Security Group. In the web UI, you can see all the targets and metrics being monitored by Prometheus\n"
},
{
	"uri": "/deploy/deploynodejs/",
	"title": "Deploy NodeJS Backend API",
	"tags": [],
	"description": "",
	"content": "Let’s bring up the NodeJS Backend API!\nCopy/Paste the following commands into your Cloud9 workspace:\ncd ~/environment/ecsdemo-nodejs kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml  We can watch the progress by looking at the deployment status:\nkubectl get deployment ecsdemo-nodejs  "
},
{
	"uri": "/eksctl/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "For this module, we need to download the eksctl binary:\ncurl --silent --location \u0026quot;https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz\u0026quot; | tar xz -C /tmp sudo mv -v /tmp/eksctl /usr/local/bin  Confirm the eksctl command works:\neksctl version  "
},
{
	"uri": "/scaling/deploy_hpa/",
	"title": "Configure Horizontal Pod AutoScaler (HPA)",
	"tags": [],
	"description": "",
	"content": " Deploy the Metrics Server Metrics Server is a cluster-wide aggregator of resource usage data. These metrics will drive the scaling behavior of the deployments. We will deploy the metrics server using Helm configured in a previous module\nhelm install stable/metrics-server \\ --name metrics-server \\ --version 2.0.2 \\ --namespace metrics  Confirm the Metrics API is available. Return to the terminal in the Cloud9 Environment\nkubectl get apiservice v1beta1.metrics.k8s.io -o yaml  If all is well, you should see a status message similar to the one below in the response\nstatus: conditions: - lastTransitionTime: 2018-10-15T15:13:13Z message: all checks passed reason: Passed status: \u0026quot;True\u0026quot; type: Available  We are now ready to scale a deployed application "
},
{
	"uri": "/logging/prereqs/",
	"title": "Configure IAM Policy for Worker Nodes",
	"tags": [],
	"description": "",
	"content": "We will be deploying Fluentd as a DaemonSet, or one pod per worker node. The fluentd log daemon will collect logs and forward to CloudWatch Logs. This will require the nodes to have permissions to send logs and create log groups and log streams. This can be accomplished with an IAM user, IAM role, or by using a tool like Kube2IAM.\nIn our example, we will create an IAM policy and attach it the the Worker node role.\nCollect the Instance Profile and Role NAME from the CloudFormation Stack\nINSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks --stack-name eksctl-eksworkshop-eksctl-nodegroup-0 | jq -r '.Stacks[].Outputs[].ExportName' | sed 's/:.*//') INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName')  Create a new IAM Policy and attach it to the Worker Node Role.\nmkdir ~/environment/iam_policy cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/iam_policy/k8s-logs-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: [ \u0026quot;logs:DescribeLogGroups\u0026quot;, \u0026quot;logs:DescribeLogStreams\u0026quot;, \u0026quot;logs:CreateLogGroup\u0026quot;, \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;logs:PutLogEvents\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot; } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker --policy-document file://~/environment/iam_policy/k8s-logs-policy.json  Validate that the policy is attached to the role\naws iam get-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker  "
},
{
	"uri": "/helm/deploy/",
	"title": "Deploy Helm",
	"tags": [],
	"description": "",
	"content": " Configure Helm access with RBAC Helm relies on a service called tiller that requires special permission on the kubernetes cluster, so we need to build a Service Account for tiller to use. We\u0026rsquo;ll then apply this to the cluster.\nTo create a new service account manifest:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system EoF  Next apply the config:\nkubectl apply -f ~/environment/rbac.yaml  Then we can install helm using the helm tooling\nhelm init --service-account tiller  This will install tiller into the cluster which gives it access to manage resources in your cluster.\n"
},
{
	"uri": "/dashboard/dashboard/",
	"title": "Deploy the Official Kubernetes Dashboard",
	"tags": [],
	"description": "",
	"content": "The official Kubernetes dashboard is not deployed by default, but there are instructions in the official documentation\nWe can deploy the dashboard with the following command:\nkubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml  Since this is deployed to our private cluster, we need to access it via a proxy. Kube-proxy is available to proxy our requests to the dashboard service. In your workspace, run the following command:\nkubectl proxy --port=8080 --address='0.0.0.0' --disable-filter=true \u0026amp;  This will start the proxy, listen on port 8080, listen on all interfaces, and will disable the filtering of non-localhost requests.\nThis command will continue to run in the background of the current terminal\u0026rsquo;s session.\nWe are disabling request filtering, a security feature that guards against XSRF attacks. This isn\u0026rsquo;t recommended for a production environment, but is useful for our dev environment.\n "
},
{
	"uri": "/codepipeline/role/",
	"title": "Create IAM Role",
	"tags": [],
	"description": "",
	"content": "In an AWS CodePipeline, we are going to use AWS CodeBuild to deploy a sample Kubernetes service. This requires an AWS Identity and Access Management (IAM) role capable of interacting with the EKS cluster.\nIn this step, we are going to create an IAM role and add an inline policy that we will use in the CodeBuild stage to interact with the EKS cluster via kubectl.\nCreate the role:\ncd ~/environment ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) TRUST=\u0026quot;{ \\\u0026quot;Version\\\u0026quot;: \\\u0026quot;2012-10-17\\\u0026quot;, \\\u0026quot;Statement\\\u0026quot;: [ { \\\u0026quot;Effect\\\u0026quot;: \\\u0026quot;Allow\\\u0026quot;, \\\u0026quot;Principal\\\u0026quot;: { \\\u0026quot;AWS\\\u0026quot;: \\\u0026quot;arn:aws:iam::$ACCOUNT_ID:root\\\u0026quot; }, \\\u0026quot;Action\\\u0026quot;: \\\u0026quot;sts:AssumeRole\\\u0026quot; } ] }\u0026quot; echo '{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;eks:Describe*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }' \u0026gt; /tmp/iam-role-policy aws iam create-role --role-name EksWorkshopCodeBuilKubectldRole --assume-role-policy-document \u0026quot;$TRUST\u0026quot; --output text --query 'Role.Arn' aws iam put-role-policy --role-name EksWorkshopCodeBuilKubectldRole --policy-name eks-describe --policy-document file:///tmp/iam-role-policy  "
},
{
	"uri": "/prerequisites/workspace/",
	"title": "Create a Workspace",
	"tags": [],
	"description": "",
	"content": "The Cloud9 workspace should be built by an IAM user with Administrator privileges, not the root account user. Please ensure you are logged in as an IAM user, not the root account user.\n This workshop was designed to run in the Oregon (us-west-2) region. Please don\u0026rsquo;t run in any other region. Future versions of this workshop will expand region availability, and this message will be removed.\n -- Ad blockers, javascript disablers, and tracking blockers should be disabled for the cloud9 domain, or connecting to the workspace might be impacted.\n  Create a Cloud9 Environment  select Create environment  Name it eksworkshop, and take all other defaults When it comes up, customize the environment by closing the welcome tab and lower work area, and opening a new terminal tab in the main work area:  Your workspace should now look like this:  If you like this theme, you can choose it yourself by selecting View / Themes / Solarized / Solarized Dark in the Cloud9 workspace menu.\n  "
},
{
	"uri": "/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": " Prerequisites for the Workshop  Create an AWS account   Create a Workspace   Create a SSH key   Install Kubernetes Tools   Clone the Service Repos   Create an IAM role for your Workspace   Attach the IAM role to your Workspace   Update IAM settings for your Workspace   "
},
{
	"uri": "/codepipeline/configmap/",
	"title": "Modify aws-auth ConfigMap",
	"tags": [],
	"description": "",
	"content": "Now that we have the IAM role created, we are going to add the role to the aws-auth ConfigMap for the EKS cluster.\nOnce the ConfigMap includes this new role, kubectl in the CodeBuild stage of the pipeline will be able to interact with the EKS cluster via the IAM role.\nACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) ROLE=\u0026quot; - rolearn: arn:aws:iam::$ACCOUNT_ID:role/EksWorkshopCodeBuilKubectldRole\\n username: build\\n groups:\\n - system:masters\u0026quot; kubectl get -n kube-system configmap/aws-auth -o yaml | awk \u0026quot;/mapRoles: \\|/{print;print \\\u0026quot;$ROLE\\\u0026quot;;next}1\u0026quot; \u0026gt; /tmp/aws-auth-patch.yml kubectl patch configmap/aws-auth -n kube-system --patch \u0026quot;$(cat /tmp/aws-auth-patch.yml)\u0026quot;  If you would like to edit the aws-auth ConfigMap manually, you can run: $ kubectl edit -n kube-system configmap/aws-auth\n "
},
{
	"uri": "/codepipeline/forksample/",
	"title": "Fork Sample Repository",
	"tags": [],
	"description": "",
	"content": "We are now going to fork the sample Kubernetes service so that we will be able modify the repository and trigger builds.\nLogin to GitHub and fork the sample service to your own account:\nhttps://github.com/rnzsgh/eks-workshop-sample-api-service-go\nOnce the repo is forked, you can view it in your your GitHub repositories.\nThe forked repo will look like:\n"
},
{
	"uri": "/codepipeline/githubcredentials/",
	"title": "GitHub Access Token",
	"tags": [],
	"description": "",
	"content": "In order for CodePipeline to receive callbacks from GitHub, we need to generate a personal access token.\nOnce created, an access token can be stored in a secure enclave and reused, so this step is only required during the first run or when you need to generate new keys.\nOpen up the New personal access page in GitHub.\nYou may be prompted to enter your GitHub password\n Enter a value for Token description, check the repo permission scope and scroll down and click the Generate token button\nCopy the personal access token and save it in a secure place for the next step\n"
},
{
	"uri": "/codepipeline/codepipeline/",
	"title": "CodePipeline Setup",
	"tags": [],
	"description": "",
	"content": "Now we are going to create the AWS CodePipeline using AWS CloudFormation.\nCloudFormation is an infrastructure as code (IaC) tool which provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts.\nEach EKS deployment/service should have its own CodePipeline and be located in an isolated source repository.\nYou can modify the CloudFormation templates provided with this workshop to meet your system requirements to easily onboard new services to your EKS cluster. For each new service the following steps can be repeated.\nClick the Launch button to create the CloudFormation stack in the AWS Management Console.\n   Launch template       CodePipeline \u0026amp; EKS  Launch    Download      After the console is open, enter your GitHub username, personal access token (created in previous step), check the acknowledge box and then click the \u0026ldquo;Create\u0026rdquo; button.\nWait for the status to change from \u0026ldquo;CREATE_IN_PROGRESS\u0026rdquo; to CREATE_COMPLETE before moving on to the next step.\nOpen CodePipeline in the Management Console. You will see a CodePipeline that starts with eks-workshop-codepipeline. Click this link to view the details.\nOnce you are on the detail page for the specific CodePipeline, you can see the status along with links to the change and build details.\nIf you click on the \u0026ldquo;details\u0026rdquo; link in the build/deploy stage, you can see the output from the CodeBuild process.\n To review the status of the deployment, you can run:\nkubectl describe deployment hello-k8s  For the status of the service, run the following command:\nkubectl describe service hello-k8s  Once the service is built and delivered, run the following command to get the Elastic Load Balancer (ELB) endpoint and open it in a browser.\nkubectl get services hello-k8s -o wide  This service was configured with a LoadBalancer so, an AWS Elastic Load Balancer is launched by Kubernetes for the service. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com\u0026rdquo; - the full value is the DNS address.\n "
},
{
	"uri": "/monitoring/deploy-grafana/",
	"title": "Deploy Grafana",
	"tags": [],
	"description": "",
	"content": " Download Grafana and update configuration curl -o grafana-values.yaml https://raw.githubusercontent.com/helm/charts/master/stable/grafana/values.yaml  Search for # storageClassName: default, uncomment and change the value to \u0026ldquo;prometheus\u0026rdquo; Search for # adminPassword: strongpassword, uncomment and change the password to \u0026ldquo;EKS!sAWSome\u0026rdquo; or something similar.\nSearch for datasources: {} and uncomment entire block, update prometheus to the endpoint referred earlier by helm response. The configuration will look similar to below\ndatasources: datasources.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://prometheus-server.prometheus.svc.cluster.local access: proxy isDefault: true  Now let\u0026rsquo;s expose Grafana dashboard using AWS ELB service. Search for service:, and update the value of type: ClusterIP to type: LoadBalancer\nDeploy grafana helm install -f grafana-values.yaml stable/grafana --name grafana --namespace grafana  Run the command to check if Grafana is running properly\nkubectl get all -n grafana  You should see similar results. They should all be Ready and Available\nNAME READY STATUS RESTARTS AGE pod/grafana-b9697f8b5-t9w4j 1/1 Running 0 2m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/grafana LoadBalancer 10.100.49.172 abe57f85de73111e899cf0289f6dc4a4-1343235144.us-west-2.elb.amazonaws.com 80:31570/TCP 3m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/grafana 1 1 1 1 2m NAME DESIRED CURRENT READY AGE replicaset.apps/grafana-b9697f8b5 1 1 1 2m  You can get Grafana ELB URL using this command. Copy \u0026amp; Paste the value into browser to access Grafana web UI\nexport ELB=$(kubectl get svc -n grafana grafana -o jsonpath='{.status.loadBalancer.ingress[0].hostname}') echo \u0026quot;http://$ELB\u0026quot;  "
},
{
	"uri": "/codepipeline/change/",
	"title": "Trigger New Release",
	"tags": [],
	"description": "",
	"content": "So far we have walked through setting up CI/CD for EKS using AWS CodePipeline and now we are going to make a change to the GitHub repository so that we can see a new release built and delivered.\nOpen GitHub and select the forked repository with the name eks-workshop-sample-api-service-go.\nClick on main.go file and then click on the edit button, which looks like a pencil.\nChange the text where it says \u0026ldquo;Hello World\u0026rdquo;, add a commit message and then click the \u0026ldquo;Commit changes\u0026rdquo; button.\nYou should leave the master branch selected.\nThe main.go application needs to be compiled, so please ensure that you don\u0026rsquo;t accidentally break the build :)\n After you modify and commit your change in GitHub, in approximately one minute you will see a new build triggered in the AWS Management Console\nOnce the service is built and delivered, run the following command to get the Elastic Load Balancer (ELB) endpoint and open it in a browser. If the message is not updated immediately, give Kubernetes some time to deploy the change.\nkubectl get services hello-k8s -o wide  The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com\u0026rdquo; - the full value is the DNS record.\n "
},
{
	"uri": "/codepipeline/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Congratulations on making it through the CI/CD with CodePipline module.\nThis module is not used in subsequent steps, so you can remove the resources now, or at the end of the workshop.\nFirst we need to delete the Kubernetes deployment and service:\nkubectl delete deployments hello-k8s kubectl delete services hello-k8s  Next, we are going to delete the CloudFormation stack created. Open up CloudFormation the AWS Managemnt Console.\nCheck the box next to the eksws-codepipeline stack, select the Actions dropdown menu and then click Delete Stack:\nNow we are going to delete the ECR respository:\nFinally, empty and then delete the S3 bucket used by CodeBuild for build artifacts (bucket name starts with eksws-codepipeline). First, you selete the bucket, then you empty the bucket and finally delete the bucket:\n"
},
{
	"uri": "/servicemesh/download/",
	"title": "Download and Install Istio CLI",
	"tags": [],
	"description": "",
	"content": "Before we can get started configuring Istio we’ll need to first install the command line tools that you will interact with. To do this run the following.\ncd ~/environment curl -L https://git.io/getLatestIstio | sh - cd istio-1.0.3 sudo mv -v bin/istioctl /usr/local/bin/  "
},
{
	"uri": "/monitoring/dashboards/",
	"title": "Dashboards",
	"tags": [],
	"description": "",
	"content": " Create Dashboards Login into Grafana dashboard using credentials supplied during configuration\nYou will notice that \u0026lsquo;Install Grafana\u0026rsquo; \u0026amp; \u0026lsquo;create your first data source\u0026rsquo; are already completed. We will import community created dashboard for this tutorial\nClick \u0026lsquo;+\u0026rsquo; button on left panel and select \u0026lsquo;Import\u0026rsquo;\nEnter 3131 dashboard id under Grafana.com Dashboard \u0026amp; click \u0026lsquo;Load\u0026rsquo;.\nLeave the defaults, select \u0026lsquo;Prometheus\u0026rsquo; as the endpoint under prometheus data sources drop down, click \u0026lsquo;Import\u0026rsquo;.\nThis will show monitoring dashboard for all cluster nodes\nFor creating dashboard to monitor all pods, repeat same process as above and enter 3146 for dashboard id\n"
},
{
	"uri": "/introduction/basics/",
	"title": "Kubernetes (k8s) Basics",
	"tags": [],
	"description": "",
	"content": "In this section, we\u0026rsquo;ll cover the following topics:\n What is Kubernetes?   Kubernetes Nodes   K8s Objects Overview   K8s Objects Detail (1/2)   K8s Objects Detail (2/2)   "
},
{
	"uri": "/deploy/deploycrystal/",
	"title": "Deploy Crystal Backend API",
	"tags": [],
	"description": "",
	"content": "Let’s bring up the Crystal Backend API!\nCopy/Paste the following commands into your Cloud9 workspace:\ncd ~/environment/ecsdemo-crystal kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml  We can watch the progress by looking at the deployment status:\nkubectl get deployment ecsdemo-crystal  "
},
{
	"uri": "/cleanup/undeploy/",
	"title": "Undeploy the applications",
	"tags": [],
	"description": "",
	"content": "To delete the resources created by the applications, we should delete the application deployments and kubernetes dashboard:\nUndeploy the applications:\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml  "
},
{
	"uri": "/eksctl/launcheks/",
	"title": "Launch EKS",
	"tags": [],
	"description": "",
	"content": "To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n "
},
{
	"uri": "/tabs-example/tabs/eksctl/",
	"title": "Launch EKS",
	"tags": [],
	"description": "",
	"content": "To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n "
},
{
	"uri": "/logging/setup_es/",
	"title": "Provision an Elasticsearch Cluster",
	"tags": [],
	"description": "",
	"content": "This example creates a two instance Amazon Elasticsearch cluster named kubernetes-logs. This cluster is created in the same region as the Kubernetes cluster and CloudWatch log group.\nNote that this cluster has an open access policy which will need to be locked down in production environments.\n aws es create-elasticsearch-domain \\ --domain-name kubernetes-logs \\ --elasticsearch-version 6.3 \\ --elasticsearch-cluster-config \\ InstanceType=m4.large.elasticsearch,InstanceCount=2 \\ --ebs-options EBSEnabled=true,VolumeType=standard,VolumeSize=100 \\ --access-policies '{\u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;,\u0026quot;Statement\u0026quot;:[{\u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;,\u0026quot;Principal\u0026quot;:{\u0026quot;AWS\u0026quot;:[\u0026quot;*\u0026quot;]},\u0026quot;Action\u0026quot;:[\u0026quot;es:*\u0026quot;],\u0026quot;Resource\u0026quot;:\u0026quot;*\u0026quot;}]}'  It takes a little while for the cluster to be created and arrive at an active state. The AWS Console should show the following status when the cluster is ready.\nYou could also check this via AWS CLI:\naws es describe-elasticsearch-domain --domain-name kubernetes-logs --query 'DomainStatus.Processing'  If the output value is false that means the domain has been processed and is now available to use.\nFeel free to move on to the next section for now.\n"
},
{
	"uri": "/scaling/test_hpa/",
	"title": "Scale an Application with HPA",
	"tags": [],
	"description": "",
	"content": " Deploy a Sample App We will deploy an application and expose as a service on TCP port 80. The application is a custom-built image based on the php-apache image. The index.php page performs calculations to generate CPU load. More information can be found here\nkubectl run php-apache --image=k8s.gcr.io/hpa-example --requests=cpu=200m --expose --port=80  Create an HPA resource This HPA scales up when CPU exceeds 50% of the allocated container resource.\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10  View the HPA using kubectl. You probably will see \u0026lt;unknown\u0026gt;/50% for 1-2 minutes and then you should be able to see 0%/50%\nkubectl get hpa  Generate load to trigger scaling Open a new terminal in the Cloud9 Environment and run the following command to drop into a shell on a new container\nkubectl run -i --tty load-generator --image=busybox /bin/sh  Execute a while loop to continue getting http:///php-apache\nwhile true; do wget -q -O - http://php-apache; done  In the previous tab, watch the HPA with the following command\nkubectl get hpa -w  You will see HPA scale the pods from 1 up to our configured maximum (10) until the CPU average is below our target (50%)\nYou can now stop (Ctrl + C) load test that was running in the other terminal. You will notice that HPA will slowly bring the replica count to min number based on its configuration. You should also get out of load testing application by pressing Ctrl + D\n"
},
{
	"uri": "/helm/using/",
	"title": "Using Helm",
	"tags": [],
	"description": "",
	"content": " Deploy our Microservices using Helm Instead of manually deploying our microservices using kubectl, we will create a custom Helm Chart. For detailed information on working with chart templates, refer to the Helm docs\nHelm charts are structured like this:\nmychart/ Chart.yaml # a description of the chart values.yaml # the default values for the chart. May be overridden during install or upgrade. charts/ # May contain subcharts templates/ # the template files themselves ...  We\u0026rsquo;ll start out by creating a new chart called eksdemo\ncd ~/environment helm create eksdemo  If you look in the the newly created eksdemo directory, you\u0026rsquo;ll see several files and directories. Inside the /templates directory, you\u0026rsquo;ll see these files.\n NOTES.txt: The “help text” for your chart. This will be displayed to your users when they run helm install. deployment.yaml: A basic manifest for creating a Kubernetes deployment service.yaml: A basic manifest for creating a service endpoint for your deployment _helpers.tpl: A place to put template helpers that you can re-use throughout the chart  We\u0026rsquo;re actually going to create our own files, so we\u0026rsquo;ll delete these boilerplate files\nrm -rf ~/environment/eksdemo/templates/*.* rm ~/environment/eksdemo/Chart.yaml rm ~/environment/eksdemo/values.yaml  Create a new Chart.yaml file which will describe the chart\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/eksdemo/Chart.yaml apiVersion: v1 appVersion: \u0026quot;1.0\u0026quot; description: A Helm chart for EKS Workshop Microservices application name: eksdemo version: 0.1.0 EoF  Now we\u0026rsquo;ll copy the manifest files for each of our microservices into the templates directory as servicename.yaml\n#create subfolders for each template type mkdir ~/environment/eksdemo/templates/deployment mkdir ~/environment/eksdemo/templates/service # Copy and rename frontend manifests cp ~/environment/ecsdemo-frontend/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/frontend.yaml cp ~/environment/ecsdemo-frontend/kubernetes/service.yaml ~/environment/eksdemo/templates/service/frontend.yaml # Copy and rename crystal manifests cp ~/environment/ecsdemo-crystal/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/crystal.yaml cp ~/environment/ecsdemo-crystal/kubernetes/service.yaml ~/environment/eksdemo/templates/service/crystal.yaml # Copy and rename nodejs manifests cp ~/environment/ecsdemo-nodejs/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/nodejs.yaml cp ~/environment/ecsdemo-nodejs/kubernetes/service.yaml ~/environment/eksdemo/templates/service/nodejs.yaml  All files in the templates directory are sent through the template engine. These are currently plain YAML files that would be sent to Kubernetes as-is.\nReplace hard-coded values with template directives Let\u0026rsquo;s replace some of the values with template directives to enable more customization and start removing hard-coded values.\nOpen ~/environment/eksdemo/templates/deployment/frontend.yaml in your Cloud9 editor. You will repeat these steps for crystal.yaml and nodejs.yaml\n Under spec, find replicas: 1 and replace with the following:\nreplicas: {{ .Values.replica }}  Under spec.template.spec.containers.image, replace the image with the correct template value from the table below:\n   Filename Value     frontend.yaml image: {{ .Values.frontend.image }}:{{ .Values.version }}   crystal.yaml image: {{ .Values.crystal.image }}:{{ .Values.version }}   nodejs.yaml image: {{ .Values.nodejs.image }}:{{ .Values.version }}    Create a values.yaml file with our template defaults This file will populate our template directives with default values.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/eksdemo/values.yaml # Default values for eksdemo. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Release-wide Values replica: 3 version: 'latest' # Service Specific Values nodejs: image: brentley/ecsdemo-nodejs crystal: image: brentley/ecsdemo-crystal frontend: image: brentley/ecsdemo-frontend EoF  Use the dry-run flag to test our templates The following command will build and output the rendered templates without installing the chart:\nhelm install --debug --dry-run --name workshop ~/environment/eksdemo  Confirm that the values created by the template look correct.\nDelete the workshop release\nhelm del --purge workshop  Deploy the chart Now that we have tested our template, lets install it.\nhelm install --name workshop ~/environment/eksdemo  Update demo application chart with a breaking change Open values.yaml and modify the image name under nodejs.image to brentley/ecsdemo-nodejs-non-existing. This image does not exist, so this will break our deployment.\nDeploy the updated demo application chart:\nhelm upgrade workshop ~/environment/eksdemo  The rolling upgrade will begin by creating a new nodejs pod with the new image. The new ecsdemo-nodejs Pod should fail to pull non-existing image. Run helm status command to see the ImagePullBackOff error:\nhelm status workshop  Rollback the failed upgrade Now we are going to rollback the application to the previous working release revision.\nFirst, list Helm release revisions:\nhelm history workshop  Then, rollback to the previous application revision (can rollback to any revision too):\n# rollback to the 1st revision helm rollback workshop 1  Validate workshop release status with:\nhelm status workshop  "
},
{
	"uri": "/eksctl/",
	"title": "Launch using eksctl",
	"tags": [],
	"description": "",
	"content": " Launch using eksctl by Weaveworks We have some very powerful partner tools that allow us to automate much of the experience of creating an EKS cluster, simplifying the process.\nIn this module, we will highlight a tool contributed by Weaveworks called eksctl, based on the official AWS CloudFormation templates, and will use it to launch and configure our EKS cluster and nodes.\n  "
},
{
	"uri": "/prerequisites/sshkey/",
	"title": "Create a SSH key",
	"tags": [],
	"description": "",
	"content": " ssh-keygen  Press enter 3 times to take the default choices\n This key will be used on the worker node instances to allow ssh access if necessary.\n"
},
{
	"uri": "/prerequisites/k8stools/",
	"title": "Install Kubernetes Tools",
	"tags": [],
	"description": "",
	"content": " Amazon EKS clusters require kubectl and kubelet binaries and the aws-iam-authenticator binary to allow IAM authentication for your Kubernetes cluster.\nIn this workshop we will give you the commands to download the Linux binaries. If you are running Mac OSX / Windows, please see the official EKS docs for the download links.\n Create the default ~/.kube directory for storing kubectl configuration mkdir -p ~/.kube  Install kubectl sudo curl --silent --location -o /usr/local/bin/kubectl \u0026quot;https://amazon-eks.s3-us-west-2.amazonaws.com/1.10.3/2018-07-26/bin/linux/amd64/kubectl\u0026quot; sudo chmod +x /usr/local/bin/kubectl  Install AWS IAM Authenticator go get -u -v github.com/kubernetes-sigs/aws-iam-authenticator/cmd/aws-iam-authenticator sudo mv ~/go/bin/aws-iam-authenticator /usr/local/bin/aws-iam-authenticator  Verify the binaries kubectl version --short --client aws-iam-authenticator help  Install JQ sudo yum -y install jq  "
},
{
	"uri": "/prerequisites/clone/",
	"title": "Clone the Service Repos",
	"tags": [],
	"description": "",
	"content": "cd ~/environment git clone https://github.com/brentley/ecsdemo-frontend.git git clone https://github.com/brentley/ecsdemo-nodejs.git git clone https://github.com/brentley/ecsdemo-crystal.git  "
},
{
	"uri": "/monitoring/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Delete Prometheus and grafana helm delete prometheus helm del --purge prometheus helm delete grafana helm del --purge grafana  "
},
{
	"uri": "/deploy/servicetype/",
	"title": "Let&#39;s check Service Types",
	"tags": [],
	"description": "",
	"content": "Before we bring up the frontend service, let\u0026rsquo;s take a look at the service types we are using: This is kubernetes/service.yaml for our frontend service:\napiVersion: v1 kind: Service metadata: name: ecsdemo-frontend spec: selector: app: ecsdemo-frontend type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 3000  Notice type: LoadBalancer: This will configure an ELB to handle incoming traffic to this service.\nCompare this to kubernetes/service.yaml for one of our backend services:\napiVersion: v1 kind: Service metadata: name: ecsdemo-nodejs spec: selector: app: ecsdemo-nodejs ports: - protocol: TCP port: 80 targetPort: 3000  Notice there is no specific service type described. When we check the kubernetes documentation we find that the default type is ClusterIP. This Exposes the service on a cluster-internal IP. Choosing this value makes the service only reachable from within the cluster.\n"
},
{
	"uri": "/prerequisites/iamrole/",
	"title": "Create an IAM role for your Workspace",
	"tags": [],
	"description": "",
	"content": " Follow this deep link to create an IAM role with Administrator access. Confirm that AWS service and EC2 are selected, then click Next to view permissions. Confirm that AdministratorAccess is checked, then click Next to review. Enter eksworkshop-admin for the Name, and select Create Role   "
},
{
	"uri": "/prerequisites/ec2instance/",
	"title": "Attach the IAM role to your Workspace",
	"tags": [],
	"description": "",
	"content": " Follow this deep link to find your Cloud9 EC2 instance Select the instance, then choose Actions / Instance Settings / Attach/Replace IAM Role  Choose eksworkshop-admin from the IAM Role drop down, and select Apply   "
},
{
	"uri": "/deploy/servicerole/",
	"title": "Ensure the ELB Service Role exists",
	"tags": [],
	"description": "",
	"content": "In AWS accounts that have never created a load balancer before, it\u0026rsquo;s possible that the service role for ELB might not exist yet.\nWe can check for the role, and create it if it\u0026rsquo;s missing.\nCopy/Paste the following commands into your Cloud9 workspace:\naws iam get-role --role-name \u0026quot;AWSServiceRoleForElasticLoadBalancing\u0026quot; || aws iam create-service-linked-role --aws-service-name \u0026quot;elasticloadbalancing.amazonaws.com\u0026quot;  "
},
{
	"uri": "/servicemesh/install/",
	"title": "Install Istio",
	"tags": [],
	"description": "",
	"content": " Install Istio\u0026rsquo;s CRD The CRD(Custom Resource Definition) API resource allows you to define custom resources. To find more about CRD click here.\nkubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml  Install Istio Make sure you have Helm to install Istio into your EKS Cluster.\nkubectl create -f install/kubernetes/helm/helm-service-account.yaml helm template install/kubernetes/helm/istio --name istio --namespace istio-system --set global.configValidation=false --set sidecarInjectorWebhook.enabled=false --set grafana.enabled=true --set servicegraph.enabled=true \u0026gt; istio.yaml kubectl create namespace istio-system kubectl apply -f istio.yaml  You can watch the progress of installation using \u0026lsquo;kubectl get pod -n istio-system -w\u0026rsquo;\nNAME READY STATUS RESTARTS AGE grafana-9cfc9d4c9-jj5qr 1\u0026frasl;1 Running 0 1m istio-citadel-6d7f9c545b-jrlnn 1\u0026frasl;1 Running 0 1m istio-cleanup-secrets-zszkd 0/1 Completed 0 1m istio-egressgateway-866885bb49-b6pjb 1\u0026frasl;1 Running 0 1m istio-galley-6d74549bb9-flzfn 1\u0026frasl;1 Running 0 1m istio-grafana-post-install-976qx 0/1 Completed 0 1m istio-ingressgateway-6c6ffb7dc8-9d6g7 1\u0026frasl;1 Running 0 1m istio-pilot-685fc95d96-s2fwz 2\u0026frasl;2 Running 0 1m istio-policy-688f99c9c4-nxv6s 2\u0026frasl;2 Running 0 1m istio-security-post-install-dnrf7 0/1 Completed 0 1m istio-telemetry-69b794ff59-gmwl5 2\u0026frasl;2 Running 0 1m prometheus-f556886b8-66drk 1\u0026frasl;1 Running 0 1m servicegraph-778f94d6f8-q77wr 1\u0026frasl;1 Running 0 1m\n"
},
{
	"uri": "/introduction/basics/what_is_k8s/",
	"title": "What is Kubernetes?",
	"tags": [],
	"description": "",
	"content": " Builds on over a decade of experience and best practices Utilizes declarative configuration and automation Draws upon a large ecosystem of tools, services, support  More information on what Kubernetes is all about can be found on the official Kubernetes website.\n"
},
{
	"uri": "/deploy/deployfrontend/",
	"title": "Deploy Frontend Service",
	"tags": [],
	"description": "",
	"content": "Let’s bring up the Ruby Frontend!\nCopy/Paste the following commands into your Cloud9 workspace:\ncd ~/environment/ecsdemo-frontend kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml  We can watch the progress by looking at the deployment status:\nkubectl get deployment ecsdemo-frontend  "
},
{
	"uri": "/cleanup/eksctl/",
	"title": "Delete the EKSCTL Cluster",
	"tags": [],
	"description": "",
	"content": "In order to delete the resources created for this EKS cluster, run the following commands:\nDelete the cluster:\neksctl delete cluster --name=eksworkshop-eksctl  The nodegroup will have to complete the deletion process before the EKS cluster can be deleted. The total process will take approximately 15 minutes, and can be monitored via the CloudFormation Console\n "
},
{
	"uri": "/eksctl/test/",
	"title": "Test the Cluster",
	"tags": [],
	"description": "",
	"content": " Confirm your Nodes:\nkubectl get nodes  Congratulations! You now have a fully working Amazon EKS Cluster that is ready to use!\n"
},
{
	"uri": "/dashboard/connect/",
	"title": "Access the Dashboard",
	"tags": [],
	"description": "",
	"content": "Now we can access the Kubernetes Dashboard\n In your Cloud9 environment, click Preview / Preview Running Application Scroll to the end of the URL and append:  /api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/  Open a New Terminal Tab and enter\naws-iam-authenticator token -i eksworkshop-eksctl --token-only  Copy the output of this command and then click the radio button next to Token then in the text field below pate the output from the last command.\nThen press Sign In.\nIf you want to see the dashboard in a full tab, click the Pop Out button, like below: "
},
{
	"uri": "/scaling/deploy_ca/",
	"title": "Configure Cluster Autoscaler (CA)",
	"tags": [],
	"description": "",
	"content": " Cluster Autoscaler for AWS provides integration with Auto Scaling groups. It enables users to choose from four different options of deployment:\n One Auto Scaling group - This is what we will use Multiple Auto Scaling groups Auto-Discovery Master Node setup  Configure the Cluster Autoscaler (CA) We have provided a manifest file to deploy the CA. Copy the commands below into your Cloud9 Terminal.\nmkdir ~/environment/cluster-autoscaler cd ~/environment/cluster-autoscaler wget https://eksworkshop.com/scaling/deploy_ca.files/cluster_autoscaler.yml  Configure the ASG We will need to provide the name of the Autoscaling Group that we want CA to manipulate. Collect the name of the Auto Scaling Group (ASG) containing your worker nodes. Record the name somewhere. We will us this later in the manifest file.\nYou can find it in the console by following this link.\nCheck the box beside the ASG and click Actions and Edit\nChange the following settings:\n Min: 2 Max: 8  Click Save\nConfigure the Cluster Autoscaler Using the file browser on the left, open cluster-autoscaler.yml\nSearch for command: and within this block, replace the placeholder text \u0026lt;AUTOSCALING GROUP NAME\u0026gt; with the ASG name that you copied in the previous step. Also, update AWS_REGION value to reflect the region you are using and Save the file.\ncommand: - ./cluster-autoscaler - --v=4 - --stderrthreshold=info - --cloud-provider=aws - --skip-nodes-with-local-storage=false - --nodes=2:8:eksctl-eksworkshop-eksctl-nodegroup-0-NodeGroup-SQG8QDVSR73G env: - name: AWS_REGION value: us-east-1  This command contains all of the configuration for the Cluster Autoscaler. The primary config is the --nodes flag. This specifies the minimum nodes (2), max nodes (8) and ASG Name.\nAlthough Cluster Autoscaler is the de facto standard for automatic scaling in K8s, it is not part of the main release. We deploy it like any other pod in the kube-system namespace, similar to other management pods.\nCreate an IAM Policy We need to configure an inline policy and add it to the EC2 instance profile of the worker nodes\nCollect the Instance Profile and Role NAME from the CloudFormation Stack\nINSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks --stack-name eksctl-eksworkshop-eksctl-nodegroup-0 | jq -r '.Stacks[].Outputs[].ExportName' | sed 's/:.*//') INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName')  mkdir ~/environment/asg_policy cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/asg_policy/k8s-asg-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;autoscaling:DescribeAutoScalingGroups\u0026quot;, \u0026quot;autoscaling:DescribeAutoScalingInstances\u0026quot;, \u0026quot;autoscaling:SetDesiredCapacity\u0026quot;, \u0026quot;autoscaling:TerminateInstanceInAutoScalingGroup\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name ASG-Policy-For-Worker --policy-document file://~/environment/asg_policy/k8s-asg-policy.json  Validate that the policy is attached to the role\naws iam get-role-policy --role-name $ROLE_NAME --policy-name ASG-Policy-For-Worker  Deploy the Cluster Autoscaler kubectl apply -f ~/environment/cluster-autoscaler/cluster_autoscaler.yml  Watch the logs\nkubectl logs -f deployment/cluster-autoscaler -n kube-system  We are now ready to scale our cluster   Related files   cluster_autoscaler.yml  (3 ko)    "
},
{
	"uri": "/logging/deploy/",
	"title": "Deploy Fluentd",
	"tags": [],
	"description": "",
	"content": "mkdir ~/environment/fluentd cd ~/environment/fluentd wget https://eksworkshop.com/logging/deploy.files/fluentd.yml  Explore the fluentd.yml to see what is being deployed. There is a link at the bottom of this page. The Fluentd log agent configuration is located in the Kubernetes ConfigMap. Fluentd will be deployed as a DaemonSet, i.e. one pod per worker node. In our case, a 3 node cluster is used and so 3 pods will be shown in the output when we deploy.\nUpdate REGION in fluentd.yml as required. It is set to us-west-2 by default.\n kubectl apply -f ~/environment/fluentd/fluentd.yml  Watch for all of the pods to change to running status\nkubectl get pods -w --namespace=kube-system  We are now ready to check that logs are arriving in CloudWatch Logs\nSelect the region that is mentioned in fluentd.yml to browse the Cloudwatch Log Group if required.\n  Related files   fluentd.yml  (5 ko)    "
},
{
	"uri": "/prerequisites/workspaceiam/",
	"title": "Update IAM settings for your Workspace",
	"tags": [],
	"description": "",
	"content": "Cloud9 normally manages IAM credentials dynamically. This isn\u0026rsquo;t currently compatible with the aws-iam-authenticator plugin, so we will disable it and rely on the IAM role instead.\n  Return to your workspace and click the sprocket, or launch a new tab to open the Preferences tab Select AWS SETTINGS Turn off AWS managed temporary credentials Close the Preferences tab  To ensure temporary credentials aren\u0026rsquo;t already in place we will also remove any existing credentials file:\nrm -vf ${HOME}/.aws/credentials  We should configure our aws cli with our current region as default:\nexport AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r .region) echo \u0026quot;export AWS_REGION=${AWS_REGION}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region   "
},
{
	"uri": "/servicemesh/deploy/",
	"title": "Deploy Sample Apps",
	"tags": [],
	"description": "",
	"content": " Sample Apps Now that we have all the resources installed for Istio, we will use sample application called BookInfo to review key capabilities of Service Mesh such as intelligent routing and review telemetry data using Prometheus \u0026amp; Grafana.\nThe Bookinfo application is broken into four separate microservices:\n productpage\n The productpage microservice calls the details and reviews microservices to populate the page.  details\n The details microservice contains book information.  reviews\n The reviews microservice contains book reviews. It also calls the ratings microservice.  ratings\n The ratings microservice contains book ranking information that accompanies a book review.   There are 3 versions of the reviews microservice:\n Version v1\n doesn’t call the ratings service.  Version v2\n calls the ratings service, and displays each rating as 1 to 5 black stars.  Version v3\n calls the ratings service, and displays each rating as 1 to 5 red stars.   Deploy Sample Apps Deploy sample apps by manually injecting istio proxy and confirm pods, services are running correctly\nkubectl apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml) kubectl get pod kubectl get svc  and define virtualservice, ingressgateway and find out ELB endpoint to get connected from your browser.\nkubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml kubectl get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' -n istio-system  This may take a minute or two, first for the Ingress to be created, and secondly for the Ingress to hook up with the services it exposes.\nOpen a browser and connect to the endpoint that you get from istio-ingressgateway to see whether sample app is working.\nYou have to add /productpage at the end of the URI in the browser to see the sample webpage\n  Iterate reloading the page and check out review section calls different versions of reviews (v1, v2, v3) each time\n "
},
{
	"uri": "/introduction/basics/concepts_nodes/",
	"title": "Kubernetes Nodes",
	"tags": [],
	"description": "",
	"content": "The machines that make up a Kubernetes cluster are called nodes.\nNodes in a Kubernetes cluster may be physical, or virtual.\nThere are two types of nodes:\n A Master-node type, which makes up the Control Plane, acts as the “brains” of the cluster.\n A Worker-node type, which makes up the Data Plane, runs the actual container images (via pods.)\n  We’ll dive deeper into how nodes interact with each other later in the presentation.\n"
},
{
	"uri": "/deploy/viewservices/",
	"title": "Find the Service Address",
	"tags": [],
	"description": "",
	"content": "Now that we have a running service that is type: LoadBalancer we need to find the ELB\u0026rsquo;s address. We can do this by using the get services operation of kubectl:\nkubectl get service ecsdemo-frontend  Notice the field isn\u0026rsquo;t wide enough to show the FQDN of the ELB. We can adjust the output format with this command:\nkubectl get service ecsdemo-frontend -o wide  If we wanted to use the data pragmatically, we can also output via json. This is an example of how we might be able to make use of json output:\nELB=$(kubectl get service ecsdemo-frontend -o json | jq -r '.status.loadBalancer.ingress[].hostname') curl -m3 -v $ELB  It will take several seconds for the ELB to become healthy and start passing traffic to the frontend pods.\n You should also be able to copy/paste the loadBalancer hostname into your browser and see the application running. Keep this tab open while we scale the services up on the next page.\n"
},
{
	"uri": "/logging/configurecwl/",
	"title": "Configure CloudWatch Logs and Kibana",
	"tags": [],
	"description": "",
	"content": " All AWS console URLs default to us-west-2. On the console, select the region that is configured as default for CLI in prerequisites module.\n Configure CloudWatch Logs Subscription CloudWatch Logs can be delivered to other services such as Amazon Elasticsearch for custom processing. This can be achieved by subscribing to a real-time feed of log events. A subscription filter defines the filter pattern to use for filtering which log events gets delivered to Elasticsearch, as well as information about where to send matching log events to.\nIn this section, we’ll subscribe to the CloudWatch log events from the fluent-cloudwatch stream from the eks/eksworkshop-eksctl log group. This feed will be streamed to the Elasticsearch cluster.\nOriginal instructions for this are available at:\nhttp://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html\nCreate Lambda Basic Execution Role\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/iam_policy/lambda.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;lambda.amazonaws.com\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] } EoF aws iam create-role --role-name lambda_basic_execution --assume-role-policy-document file://~/environment/iam_policy/lambda.json aws iam attach-role-policy --role-name lambda_basic_execution --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole  Go to the CloudWatch Logs console\nSelect the log group /eks/eksworkshop-eksctl/containers. Click on Actions and select Stream to Amazon ElasticSearch Service. Select the ElasticSearch Cluster kubernetes-logs and IAM role lambda_basic_execution\nClick Next\nSelect Common Log Format and click Next\nReview the configuration. Click Next and then Start Streaming\nCloudwatch page is refreshed to show that the filter was successfully created\nConfigure Kibana In Amazon Elasticsearch console, select the Elasticsearch cluster\nOpen the Kibana dashboard from the link. After a few minutes, records will begin to be indexed by ElasticSearch. You\u0026rsquo;ll need to configure an index patterns in Kibana.\nSet Index Pattern as cwl-* and click Next\nSelect @timestamp from the dropdown list and select Create index pattern\nClick on Discover and explore your logs\n"
},
{
	"uri": "/scaling/test_ca/",
	"title": "Scale a Cluster with CA",
	"tags": [],
	"description": "",
	"content": " Deploy a Sample App We will deploy an sample nginx application as a ReplicaSet of 1 Pod\ncat \u0026lt;\u0026lt;EoF\u0026gt; ~/environment/cluster-autoscaler/nginx.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-to-scaleout spec: replicas: 1 template: metadata: labels: service: nginx app: nginx spec: containers: - image: nginx name: nginx-to-scaleout resources: limits: cpu: 500m memory: 512Mi requests: cpu: 500m memory: 512Mi EoF kubectl apply -f ~/environment/cluster-autoscaler/nginx.yaml kubectl get deployment/nginx-to-scaleout  Scale our ReplicaSet OK, let\u0026rsquo;s scale out the replicaset to 10\nkubectl scale --replicas=10 deployment/nginx-to-scaleout  Some pods will be in the Pending state, which triggers the cluster-autoscaler to scale out the EC2 fleet.\nkubectl get pods -o wide --watch  NAME READY STATUS RESTARTS AGE nginx-to-scaleout-7cb554c7d5-2d4gp 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-2nh69 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-45mqz 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-4qvzl 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-5jddd 1/1 Running 0 34s nginx-to-scaleout-7cb554c7d5-5sx4h 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-5xbjp 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-6l84p 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-7vp7l 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-86pr6 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-88ttw 0/1 Pending 0 12s  View the cluster-autoscaler logs\nkubectl logs -f deployment/cluster-autoscaler -n kube-system  You will notice Cluster Autoscaler events similar to below Check the AWS Management Console to confirm that the Auto Scaling groups are scaling up to meet demand. This may take a few minutes. You can also follow along with the pod deployment from the command line. You should see the pods transition from pending to running as nodes are scaled up.\n"
},
{
	"uri": "/servicemesh/routing/",
	"title": "Intelligent Routing",
	"tags": [],
	"description": "",
	"content": " Intelligent Routing  Deploying a microservice-based application in an Istio service mesh allows one to externally control service monitoring and tracing, request (version) routing, resiliency testing, security and policy enforcement, etc., In a consistent way across the services, for the application as a whole.\n Before you can use Istio to control the Bookinfo version routing, you need to define the available versions, called subsets, in destination rules\nService versions (a.k.a. subsets) - In a continuous deployment scenario, for a given service, there can be distinct subsets of instances running different variants of the application binary. These variants are not necessarily different API versions. They could be iterative changes to the same service, deployed in different environments (prod, staging, dev, etc.). Common scenarios where this occurs include A/B testing, canary rollouts, etc. The choice of a particular version can be decided based on various criterion (headers, url, etc.) and/or by weights assigned to each version. Each service has a default version consisting of all its instances.\nkubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml kubectl get destinationrules -o yaml  To route to one version only, you apply virtual services that set the default version for the microservices. In this case, the virtual services will route all traffic to reviews:v1 of each microservice.\nkubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml kubectl get virtualservices reviews -o yaml   Iterate reloading the page and check out review section calls only version of reviews v1 all the time\n Change the route configuration so that all traffic from a specific user is routed to a specific service version. In this case, all traffic from a user named Jason will be routed to the service reviews:v2.\nkubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml kubectl get virtualservices reviews -o yaml   Click Sign in on top right corner and login using \u0026lsquo;jason\u0026rsquo; as user name with blank password. You will only see reviews:v2 all the time. Others will see reviews:v1\n To test for resiliency, inject a 7s delay between the reviews:v2 and ratings microservices for user jason. This test will uncover a bug that was intentionally introduced into the Bookinfo app.\nkubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-delay.yaml kubectl get virtualservice ratings -o yaml   Click Sign in on top right corner and login using \u0026lsquo;jason\u0026rsquo; as user name with blank password. You will see the delays and it ends up display error for reviews. Others will see reviews without error. The timeout between the productpage and the reviews service is 6 seconds - coded as 3s + 1 retry for 6s total.\n To test for another resiliency, introduce an HTTP abort to the ratings microservices for the test user jason. The page will immediately display the “Ratings service is currently unavailable”\nkubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-abort.yaml kubectl get virtualservice ratings -o yaml   Click Sign in on top right corner and login using \u0026lsquo;jason\u0026rsquo; as user name with blank password. You will see error for ratings. Others will see rating without error.\n This demo shows you how to gradually migrate traffic from one version of a microservice to another. Send 50% of traffic to reviews:v1 and 50% to reviews:v3.\nkubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml kubectl get virtualservice reviews -o yaml   keep refreshing your browser. You will see the pages for reviews:v1 and reviews:v3\n "
},
{
	"uri": "/introduction/basics/concepts_objects/",
	"title": "K8s Objects Overview",
	"tags": [],
	"description": "",
	"content": "Kubernetes objects are entities that are used to represent the state of the cluster.\nAn object is a “record of intent” – once created, the cluster does its best to ensure it exists as defined. This is known as the cluster’s “desired state.”\nKubernetes is always working to make an object’s “current state” equal to the object’s “desired state.” A desired state can describe:\n What pods (containers) are running, and on which nodes IP endpoints that map to a logical group of containers How many replicas of a container are running And much more\u0026hellip;  Let’s explain these k8s objects in a bit more detail\u0026hellip;\n"
},
{
	"uri": "/deploy/scalebackend/",
	"title": "Scale the Backend Services",
	"tags": [],
	"description": "",
	"content": "When we launched our services, we only launched one container of each. We can confirm this by viewing the running pods:\nkubectl get deployments  Now let\u0026rsquo;s scale up the backend services:\nkubectl scale deployment ecsdemo-nodejs --replicas=3 kubectl scale deployment ecsdemo-crystal --replicas=3  Confirm by looking at deployments again:\nkubectl get deployments  Also, check the browser tab where we can see our application running. You should now see traffic flowing to multiple backend services.\n"
},
{
	"uri": "/logging/cleanup/",
	"title": "Cleanup Logging",
	"tags": [],
	"description": "",
	"content": "INSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks --stack-name eksctl-eksworkshop-eksctl-nodegroup-0 | jq -r '.Stacks[].Outputs[].ExportName' | sed 's/:.*//') INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') kubectl delete -f ~/environment/fluentd/fluentd.yml rm -rf ~/environment/fluentd/ aws es delete-elasticsearch-domain --domain-name kubernetes-logs aws logs delete-log-group --log-group-name /eks/eksworkshop-eksctl/containers aws iam delete-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker aws iam detach-role-policy --role-name lambda_basic_execution --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole aws iam delete-role --role-name lambda_basic_execution rm -rf ~/environment/iam_policy/  "
},
{
	"uri": "/scaling/cleanup/",
	"title": "Cleanup Scaling",
	"tags": [],
	"description": "",
	"content": "INSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks --stack-name eksctl-eksworkshop-eksctl-nodegroup-0 | jq -r '.Stacks[].Outputs[].ExportName' | sed 's/:.*//') INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') kubectl delete -f ~/environment/cluster-autoscaler/cluster_autoscaler.yml kubectl delete -f ~/environment/cluster-autoscaler/nginx.yaml rm -rf ~/environment/cluster-autoscaler aws iam delete-role-policy --role-name $ROLE_NAME --policy-name ASG-Policy-For-Worker kubectl delete hpa,svc php-apache kubectl delete deployment php-apache load-generator  "
},
{
	"uri": "/cleanup/workspace/",
	"title": "Cleanup the Workspace",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s delete our SSH key:\naws ec2 delete-key-pair --key-name \u0026quot;eksworkshop\u0026quot;  Since we no longer need the Cloud9 instance to have Administrator access to our account, we can delete the role we created:\n Go to the IAM Console Click Delete role in the upper right corner  Finally, let\u0026rsquo;s delete our Cloud9 EC2 Instance:\n Go to your Cloud9 Environment Select the environment named eksworkshop and pick delete  "
},
{
	"uri": "/dashboard/",
	"title": "Deploy the Kubernetes Dashboard",
	"tags": [],
	"description": "",
	"content": " Deploy the Kubernetes Dashboard In this Chapter, we will deploy the official Kubernetes dashboard, and connect through our Cloud9 Workspace.\n"
},
{
	"uri": "/servicemesh/visualize/",
	"title": "Monitor &amp; Visualize",
	"tags": [],
	"description": "",
	"content": " Collecting new telemetry data Download a YAML file to hold configuration for the new metric and log stream that Istio will generate and collect automatically.\ncurl -LO https://s3.ap-northeast-2.amazonaws.com/docshared/AWS/istio-telemetry.yaml kubectl apply -f istio-telemetry.yaml  Make sure prometheus and grafana is running\nkubectl -n istio-system get svc prometheus kubectl -n istio-system get svc grafana  Setup port-forwarding for Grafana by executing the following command:\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 8080:3000 \u0026amp;  Open the Istio Dashboard via the Grafana UI\n In your Cloud9 environment, click Preview / Preview Running Application Scroll to the end of the URL and append:  /dashboard/db/istio-mesh-dashboard  Open a new terminal tab and enter to send a traffic to the mesh\nexport SMHOST=$(kubectl get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname} ' -n istio-system) SMHOST=\u0026quot;$(echo -e \u0026quot;${SMHOST}\u0026quot; | tr -d '[:space:]')\u0026quot; while true; do curl -o /dev/null -s \u0026quot;${SMHOST}/productpage\u0026quot;; done  You will see that the traffic is evenly spread between reviews:v1 and reviews:v3\nWe encourage you to review other Istio dashboards that are available by clicking Istio Mesh Dashboard Menu on top left and selecting different dashboard\n"
},
{
	"uri": "/introduction/basics/concepts_objects_details_1/",
	"title": "K8s Objects Detail (1/2)",
	"tags": [],
	"description": "",
	"content": " Pod  A thin wrapper around one or more containers  DaemonSet  Implements a single instance of a pod on a worker node  Deployment  Details how to roll out (or roll back) across versions of your application  "
},
{
	"uri": "/deploy/scalefrontend/",
	"title": "Scale the Frontend",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s also scale our frontend service the same way:\nkubectl get deployments kubectl scale deployment ecsdemo-frontend --replicas=3 kubectl get deployments  Check the browser tab where we can see our application running. You should now see traffic flowing to multiple frontend services.\n"
},
{
	"uri": "/deploy/",
	"title": "Deploy the Example Microservices",
	"tags": [],
	"description": "",
	"content": " Deploy the Example Microservices  Deploy our Sample Applications   Deploy NodeJS Backend API   Deploy Crystal Backend API   Let\u0026#39;s check Service Types   Ensure the ELB Service Role exists   Deploy Frontend Service   Find the Service Address   Scale the Backend Services   Scale the Frontend   Cleanup the applications   "
},
{
	"uri": "/conclusion/survey/",
	"title": "Let us know what you think!",
	"tags": [],
	"description": "",
	"content": " Please take our survey! (function(t,e,s,n){var o,a,c;t.SMCX=t.SMCX||[],e.getElementById(n)||(o=e.getElementsByTagName(s),a=o[o.length-1],c=e.createElement(s),c.type=\"text/javascript\",c.async=!0,c.id=n,c.src=[\"https:\"===location.protocol?\"https://\":\"http://\",\"widget.surveymonkey.com/collect/website/js/tRaiETqnLgj758hTBazgd_2BU860jlhPrsKW9DSM0aec7fijRMWQEdDb7y2zM_2FUrIx.js\"].join(\"\"),a.parentNode.insertBefore(c,a))})(window,document,\"script\",\"smcx-sdk\");Create your own user feedback survey    "
},
{
	"uri": "/helm/",
	"title": "Deploy Helm",
	"tags": [],
	"description": "",
	"content": " In this Chapter, we will deploy Helm the Kubernetes Package Manager.\nKubernetes Helm Helm is a package manager for Kubernetes that packages multiple Kubernetes resources into a single logical deployment unit called Chart.\nHelm not only is a package manager, but also a Kubernetes application deployment management tool. It helps you to:\n achieve a simple (one command) and repeatable deployment manage application dependency, using specific versions of other application and services manage multiple deployment configurations: test, staging, production and others execute post/pre deployment jobs during application deployment update/rollback and test application deployments  "
},
{
	"uri": "/codepipeline/",
	"title": "CI/CD with CodePipeline",
	"tags": [],
	"description": "",
	"content": " CI/CD with CodePipeline Continuous integration (CI) and continuous delivery (CD) are essential for deft organizations. Teams are more productive when they can make discrete changes frequently, release those changes programmatically and deliver updates without disruption.\nIn this module, we will build a CI/CD pipeline using AWS CodePipeline. The CI/CD pipeline will deploy a sample Kubernetes service, we will make a change to the GitHub repository and observe the automated delivery of this change to the cluster.\n"
},
{
	"uri": "/logging/",
	"title": "Logging with Elasticsearch, Fluentd, and Kibana (EFK)",
	"tags": [],
	"description": "",
	"content": " Implement Logging with EFK In this Chapter, we will deploy a common Kubernetes logging pattern which consists of the following:\n Fluentd is an open source data collector providing a unified logging layer, supported by 500+ plugins connecting to many types of systems. Elasticsearch is a distributed, RESTful search and analytics engine. Kibana lets you visualize your Elasticsearch data.  Together, Fluentd, Elasticsearch and Kibana is also known as “EFK stack”. Fluentd will forward logs from the individual instances in the cluster to a centralized logging backend (CloudWatch Logs) where they are combined for higher-level reporting using ElasticSearch and Kibana.\n"
},
{
	"uri": "/scaling/",
	"title": "Autoscaling our Applications and Clusters",
	"tags": [],
	"description": "",
	"content": " Implement AutoScaling with HPA and CA In this Chapter, we will show patterns for scaling your worker nodes and applications deployments automatically. Automatic scaling in K8s comes in two forms:\n Horizontal Pod Autoscaler (HPA) scales the pods in a deployment or replica set. It is implemented as a K8s API resource and a controller. The controller manager queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. It obtains the metrics from either the resource metrics API (for per-pod resource metrics), or the custom metrics API (for all other metrics).\n Cluster Autoscaler (CA) is the default K8s component that can be used to perform pod scaling as well as scaling nodes in a cluster. It automatically increases the size of an Auto Scaling group so that pods have a place to run. And it attempts to remove idle nodes, that is, nodes with no running pods.\n  "
},
{
	"uri": "/monitoring/",
	"title": "Monitoring using Prometheus and Grafana",
	"tags": [],
	"description": "",
	"content": " Monitoring using Prometheus and Grafana In this Chapter, we will deploy Prometheus and Grafana to monitor Kubernetes cluster\n"
},
{
	"uri": "/servicemesh/",
	"title": "Servicemesh with Istio",
	"tags": [],
	"description": "",
	"content": " Service Mesh  A service mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application\n Sservice mesh solution have two distinct components that behave somewhat differently: a data plane and a control plane. Below is an presents the basic architecture.\n The data plane is composed of a set of intelligent proxies (Envoy) deployed as sidecars. These proxies mediate and control all network communication between microservices along with Mixer, a general-purpose policy and telemetry hub.\n The control plane manages and configures the proxies to route traffic. Additionally, the control plane configures Mixers to enforce policies and collect telemetry.\n  "
},
{
	"uri": "/servicemesh/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Remove telemetry configuration / port-forward process kubectl delete -f istio-telemetry.yaml killall kubectl  Remove the application virtual services / destination rules kubectl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml kubectl delete -f samples/bookinfo/networking/destination-rule-all.yaml  Remove the gateway / application kubectl delete -f samples/bookinfo/networking/bookinfo-gateway.yaml kubectl delete -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml)  Remove the Istio kubectl delete -f istio.yaml kubectl delete -f install/kubernetes/helm/helm-service-account.yaml kubectl delete -f install/kubernetes/helm/istio/templates/crds.yaml  "
},
{
	"uri": "/introduction/basics/concepts_objects_details_2/",
	"title": "K8s Objects Detail (2/2)",
	"tags": [],
	"description": "",
	"content": " ReplicaSet  Ensures a defined number of pods are always running  Job  Ensures a pod properly runs to completion  Service  Maps a fixed IP address to a logical group of pods  Label  Key/Value pairs used for association and filtering  "
},
{
	"uri": "/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Cleanup "
},
{
	"uri": "/introduction/architecture/",
	"title": "Kubernetes Architecture",
	"tags": [],
	"description": "",
	"content": "In this section, we\u0026rsquo;ll cover the following topics:\n Architectural Overview   Control Plane   Data Plane   Kubernetes Cluster Setup   "
},
{
	"uri": "/introduction/architecture/architecture_control_and_data_overview/",
	"title": "Architectural Overview",
	"tags": [],
	"description": "",
	"content": "graph TB internet((internet)) kubectl{kubectl} subgraph ControlPlane api(API Server) controller(Controller Manager) scheduler(Scheduler) etcd(etcd) end subgraph worker1 kubelet1(kubelet) kube-proxy1(kube-proxy) subgraph docker1 subgraph podA containerA[container] end subgraph podB containerB[container] end end end internet--kube-proxy1 kubectl--api controller--api scheduler--api api--kubelet1 api--etcd kubelet1--containerA kubelet1--containerB kube-proxy1--containerA kube-proxy1--containerB classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;  "
},
{
	"uri": "/introduction/eks/",
	"title": "Amazon EKS",
	"tags": [],
	"description": "",
	"content": "In this section, we\u0026rsquo;ll cover the following topics:\n EKS Cluster Creation Workflow   What happens when you create your EKS cluster   EKS Architecture for Control plane and Worker node communication   High Level   Amazon EKS!   "
},
{
	"uri": "/deploy/cleanup/",
	"title": "Cleanup the applications",
	"tags": [],
	"description": "",
	"content": "To delete the resources created by the applications, we should delete the application deployments:\nUndeploy the applications:\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml  "
},
{
	"uri": "/conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": " Conclusion "
},
{
	"uri": "/introduction/architecture/architecture_control/",
	"title": "Control Plane",
	"tags": [],
	"description": "",
	"content": "graph TB kubectl{kubectl} subgraph ControlPlane api(API Server) controller(Controller Manager) scheduler(Scheduler) etcd(etcd) end kubectl--api controller--api scheduler--api api--kubelet api--etcd classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;   One or More API Servers: Entry point for REST / kubectl\n etcd: Distributed key/value store\n Controller-manager: Always evaluating current vs desired state\n Scheduler: Schedules pods to worker nodes\n  Check out the official Kubernetes documentation for a more in-depth explanation of control plane components.\n"
},
{
	"uri": "/introduction/architecture/architecture_worker/",
	"title": "Data Plane",
	"tags": [],
	"description": "",
	"content": "graph TB internet((internet)) subgraph worker1 kubelet1(kubelet) kube-proxy1(kube-proxy) subgraph docker1 subgraph podA containerA[container] end subgraph podB containerB[container] end end end internet--kube-proxy1 api--kubelet1 kubelet1--containerA kubelet1--containerB kube-proxy1--containerA kube-proxy1--containerB classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;   Made up of worker nodes\n kubelet: Acts as a conduit between the API server and the node\n kube-proxy: Manages IP translation and routing\n  Check out the official Kubernetes documentation for a more in-depth explanation of data plane components.\n"
},
{
	"uri": "/introduction/architecture/cluster_setup_options/",
	"title": "Kubernetes Cluster Setup",
	"tags": [],
	"description": "",
	"content": "In addition to the managed Amazon EKS solution, there are many tools available to help bootstrap and configure a self-managed Kubernetes cluster. They include:\n Minikube – Development and Learning Kops – Learning, Development, Production Kubeadm – Learning, Development, Production Docker for Mac - Learning, Development  Alongside these open source solutions, there are also many commercial options available.\nLet\u0026rsquo;s take a look at Amazon EKS!\n"
},
{
	"uri": "/introduction/eks/eks_customers/",
	"title": "EKS Cluster Creation Workflow",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/eks/eks_control_plane/",
	"title": "What happens when you create your EKS cluster",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/eks/eks_high_architecture/",
	"title": "EKS Architecture for Control plane and Worker node communication",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/eks/eks_high_level/",
	"title": "High Level",
	"tags": [],
	"description": "",
	"content": "Once your EKS cluster is ready, you get an API endpoint and you\u0026rsquo;d use Kubectl, community developed tool to interact with your cluster.\n"
},
{
	"uri": "/introduction/eks/stay_tuned/",
	"title": "Amazon EKS!",
	"tags": [],
	"description": "",
	"content": "Stay tuned as we continue the journey with EKS in the next module!\nAlways ask questions! Feel free to ask them in person during this workshop, or any time on the official Kubernetes Slack channel accessible via http://slack.k8s.io/.\n"
},
{
	"uri": "/tabs-example/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Tabs with regular text:  Tab 1 Tab 2  echo \"This is tab 1.\"  println \"This is tab 2.\"   $(function(){$(\"#tab\").tabs();}); Tabs with code blocks:  Tab 1 Tab 2  echo \u0026#34;This is tab 1.\u0026#34;   println \u0026#34;This is tab 2.\u0026#34;    $(function(){$(\"#tab-with-code\").tabs();}); Tabs showing installation process:  eksctl terraform cloudformation  To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n  We start by initializing the Terraform state:\nterraform init  We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n  To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text)  Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS}  Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text  When your cluster status is ACTIVE you can proceed.\n  $(function(){$(\"#tab-installation\").tabs();}); Second set of tabs showing installation process:  eksctl terraform cloudformation  To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n  We start by initializing the Terraform state:\nterraform init  We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n  To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text)  Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS}  Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text  When your cluster status is ACTIVE you can proceed.\n  $(function(){$(\"#more-tab-installation\").tabs();}); "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/authors/",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": " Thanks to our wonderful contributors  for making Open Source a better place! .ghContributors{ display:flex; flex-flow: wrap; align-content: flex-start } .ghContributors  div{ width: 50% ; display: inline-flex; margin-bottom: 5px; } .ghContributors  div label{ padding-left: 4px ; } .ghContributors  div span{ font-size: x-small; padding-left: 4px ; }   @brentley 31 commits   @jpeddicord 6 commits   @oak2278 5 commits   @geremyCohen 1 commits   @kimsaabyepedersen 1 commits   @dalbhanj 1 commits   @ranrotx 1 commits   @rnzsgh 1 commits   @shaiss 1 commits   @sajee 1 commits   @algestam 1 commits   @jaybarnes 1 commits   "
},
{
	"uri": "/tabs-example/tabs/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tabs-example/tabs/eks/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": "To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text)  Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS}  Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text  When your cluster status is ACTIVE you can proceed.\n"
},
{
	"uri": "/tabs-example/tabs/launcheks/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": "We start by initializing the Terraform state:\nterraform init  We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n "
},
{
	"uri": "/example_cf_templates/",
	"title": "Example of using CloudFormation Templates",
	"tags": [],
	"description": "",
	"content": " Click below to add a CloudFormation Stack    Use these templates:       Template 1 example  Launch    Download     Template 2 example  Launch    Download     Template 3 example  Launch    Download      "
},
{
	"uri": "/more_resources/",
	"title": "More Resources",
	"tags": [],
	"description": "",
	"content": " Discover more AWS resources for building and running your application on AWS:\nMore Workshops  Amazon ECS Workshop - Learn how to use Stelligent Mu to deploy a microservice architecture that runs in AWS Fargate Amazon Lightsail Workshop - If you are getting started with the cloud and looking for a way to run an extremely low cost environment Lightsail is perfect. Learn how to deploy to Amazon Lightsail with this workshop.  Tools for AWS Fargate and Amazon ECS  Containers on AWS - Learn common best-practices for running containers on AWS fargate - Command line tool for interacting with AWS Fargate. With just a single command you can build, push, and launch your container in Fargate, orchestrated by ECS. Terraform - Use Terraform to deploy your docker containers in Fargate Wonqa is a tool for spinning up disposable QA environments in AWS Fargate, with SSL enabled by Let\u0026rsquo;s Encrypt. More details about Wonqa on the Wonder Engineering blog coldbrew - Fantastic tool that provisions ECS infrastructure, builds and deploys your container, and connects your services to an application load balancer automatically. Has a great developer experience for day to day use mu - Automates everything relating to ECS devops and CI/CD. This framework lets you write a simple metadata file and it constructs all the infrastructure you need so that you can deploy to ECS by simply pushing to your Git repo.  Courses  Microservices with Docker, Flask, and React - Learn how to build, test, and deploy microservices powered by Docker, Flask, React Amazon ECS!  "
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]